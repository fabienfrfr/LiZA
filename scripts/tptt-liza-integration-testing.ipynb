{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q -U git+https://github.com/fabienfrfr/tptt@main\n",
    "#!python -V\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Llama / Llama 2/3\t`self_attn ou attention`\tq_proj, k_proj, v_proj, o_proj\n",
    "- GPT-2/3/Neo/J\t`attn ou attention`\tc_attn, c_proj (GPT-2), q_proj, etc.\n",
    "- Mistral\t`self_attn`\tq_proj, k_proj, v_proj, o_proj\n",
    "- OpenELM\t`attn`\tqkv_proj, out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hidden_states.shape: torch.Size([2, 256, 2048])\n",
    "attention_mask.shape: torch.Size([2, 1, 256, 256])\n",
    "v.shape: torch.Size([2, 256, 256])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabien/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tptt\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Config et modèle\n",
    "# config = tptt.TpttConfig(base_model_name=\"gpt2\") # Target modules {'v_proj', 'q_proj'} not found in the base model.\n",
    "# config = tptt.TpttConfig(base_model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\") # Minimum 5GB RAM (if not quantized, else 500MB)\n",
    "config = tptt.TpttConfig(base_model_name=\"apple/OpenELM-270M\", base_tokenizer_name=\"hf-internal-testing/llama-tokenizer\")\n",
    "model = tptt.TpttModel(config)\n",
    "\n",
    "# 2. (Optionnel) Injection LoRA\n",
    "model.add_lora()\n",
    "\n",
    "# 3. Préparation du dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")[\"train\"].select(range(100))\n",
    "# (instruction_format est importé via tptt)\n",
    "dataset = dataset.map(tptt.instruction_format)\n",
    "\n",
    "# 4. Entraînement\n",
    "trainer = tptt.TpttTrainer(model, dataset)\n",
    "trainer.train()\n",
    "\n",
    "# 5. Génération\n",
    "pipe = tptt.TpttPipeline(model)\n",
    "print(pipe(\"Once upon a time,\"))\n",
    "\n",
    "# 6. Sauvegarde\n",
    "model.save_pretrained(\"./my_tptt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module implements the TPTT model with linear attention and LoRA support.\"\"\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          DataCollatorWithPadding, Pipeline, PretrainedConfig,\n",
    "                          PreTrainedModel, Trainer, TrainingArguments)\n",
    "\n",
    "from .injection import inject_linear_attention\n",
    "from .liza.memory_gate import LiZAttention\n",
    "from .tuner import AdjustMaGWeightCallback\n",
    "from .utils import Cache, instruction_format\n",
    "\n",
    "\n",
    "class TpttConfig(PretrainedConfig):\n",
    "    \"\"\"Configuration class for the TPTT model.\"\"\"\n",
    "\n",
    "    model_type = \"tptt\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"gpt2\",\n",
    "        base_tokenizer_name=None,\n",
    "        target_modules_names=None,\n",
    "        operator_mode=\"delta_rule\",\n",
    "        mag_weight=0.5,\n",
    "        max_chunk_size=64,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize TpttConfig with model and attention parameters.\n",
    "\n",
    "        Args:\n",
    "            base_model_name (str): Name of the base model.\n",
    "            base_tokenizer_name (str): Name of the tokenizer.\n",
    "            target_modules_names (list): List of module name suffixes to target.\n",
    "            operator_mode (str): Operator mode for attention.\n",
    "            mag_weight (float): Weight for MaG.\n",
    "            max_chunk_size (int): Maximum chunk size for attention.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        if target_modules_names is None:\n",
    "            target_modules_names = [\"attn\", \"self_attn\", \"attention\"]\n",
    "        self.base_model_name = base_model_name\n",
    "        self.base_tokenizer_name = (\n",
    "            base_model_name if base_tokenizer_name is None else base_tokenizer_name\n",
    "        )\n",
    "        self.target_modules_names = target_modules_names\n",
    "        self.operator_mode = operator_mode\n",
    "        self.mag_weight = mag_weight\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "\n",
    "\n",
    "class TpttModel(PreTrainedModel):\n",
    "    \"\"\"TPTT model wrapper with linear attention and LoRA support.\"\"\"\n",
    "\n",
    "    config_class = TpttConfig\n",
    "\n",
    "    def __init__(self, config: TpttConfig):\n",
    "        \"\"\"\n",
    "        Initialize TpttModel.\n",
    "\n",
    "        Args:\n",
    "            config (TpttConfig): Model configuration.\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.base_model_name,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"eager\",  # compatible with Liza\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.base_tokenizer_name, trust_remote_code=True\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            if self.tokenizer.eos_token is not None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            else:\n",
    "                self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        self.cache = Cache()\n",
    "        self._inject_liza_attention()\n",
    "\n",
    "    def _inject_liza_attention(self):\n",
    "        \"\"\"Inject LiZAttention into target modules.\"\"\"\n",
    "        target_modules = [\n",
    "            name\n",
    "            for name, _ in self.model.named_modules()\n",
    "            if any(name.endswith(suffix) for suffix in self.config.target_modules_names)\n",
    "        ]\n",
    "        if not target_modules:\n",
    "            raise ValueError(\n",
    "                f\"Target modules '{self.config.target_modules_names}' not found in the model.\"\n",
    "            )\n",
    "        self.model, self.cache = inject_linear_attention(\n",
    "            self.model,\n",
    "            self.model.config,\n",
    "            liza_attention=LiZAttention,\n",
    "            target_modules=target_modules,\n",
    "            operator_mode=self.config.operator_mode,\n",
    "            mag_weight=self.config.mag_weight,\n",
    "            max_chunk_size=self.config.max_chunk_size,\n",
    "        )\n",
    "\n",
    "    def add_lora(self, lora_config: LoraConfig = None):\n",
    "        \"\"\"\n",
    "        Add LoRA adapters to the model.\n",
    "\n",
    "        Args:\n",
    "            lora_config (LoraConfig, optional): LoRA configuration.\n",
    "        \"\"\"\n",
    "        if lora_config is None:\n",
    "            candidate_names = [\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",  # Llama, Mistral, OLMo\n",
    "                \"qkv_proj\",\n",
    "                \"out_proj\",  # OpenELM, some GPTs\n",
    "                \"c_attn\",\n",
    "                \"c_proj\",  # GPT-2\n",
    "            ]\n",
    "            target_modules = [\n",
    "                name\n",
    "                for name, _ in self.model.named_modules()\n",
    "                if any(name.endswith(n) for n in candidate_names)\n",
    "            ]\n",
    "            target_modules = list(set(target_modules))\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                target_modules=target_modules,\n",
    "            )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def save_pretrained(self, path: str, **kwargs):\n",
    "        \"\"\"Save model, tokenizer, and config to the given path.\"\"\"\n",
    "        self.model.save_pretrained(path, **kwargs)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        self.config.save_pretrained(path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str, **kwargs):\n",
    "        \"\"\"Load model, tokenizer, and config from the given path.\"\"\"\n",
    "        config = TpttConfig.from_pretrained(path)\n",
    "        obj = cls(config)\n",
    "        obj.model = AutoModelForCausalLM.from_pretrained(path, **kwargs)\n",
    "        obj.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        return obj\n",
    "\n",
    "\n",
    "class TpttTrainer:\n",
    "    \"\"\"Trainer for TPTT models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: TpttModel,\n",
    "        tokenized_dataset=None,\n",
    "        training_args=None,\n",
    "        initial_weight=0.01,\n",
    "        final_weight=0.5,\n",
    "        transition_step=500,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize TpttTrainer.\n",
    "\n",
    "        Args:\n",
    "            model (TpttModel): The TPTT model.\n",
    "            tokenized_dataset (Dataset, optional): Pre-tokenized dataset.\n",
    "            training_args (TrainingArguments, optional): Training arguments.\n",
    "            initial_weight (float): Initial MaG weight.\n",
    "            final_weight (float): Final MaG weight.\n",
    "            transition_step (int): Transition step for MaG weight.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = model.tokenizer\n",
    "\n",
    "        if tokenized_dataset is None:\n",
    "            raw_dataset = (\n",
    "                load_dataset(\"yahma/alpaca-cleaned\")[\"train\"]\n",
    "                .select(range(1000))\n",
    "                .map(instruction_format)\n",
    "            )\n",
    "            self.tokenized_dataset = raw_dataset.map(\n",
    "                self.tokenize, batched=True, remove_columns=raw_dataset.column_names\n",
    "            )\n",
    "        else:\n",
    "            self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            self.tokenizer, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )  # padding=\"longest\"\n",
    "\n",
    "        self.training_args = training_args or TrainingArguments(\n",
    "            output_dir=\"./tptt_output\",\n",
    "            per_device_train_batch_size=2,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            report_to=\"tensorboard\",\n",
    "        )\n",
    "\n",
    "        self.liza_callback = AdjustMaGWeightCallback(\n",
    "            self.model.model,\n",
    "            initial_weight=initial_weight,\n",
    "            final_weight=final_weight,\n",
    "            transition_step=transition_step,\n",
    "        )\n",
    "\n",
    "    def tokenize(self, samples):\n",
    "        \"\"\"\n",
    "        Tokenize samples in batch.\n",
    "\n",
    "        Args:\n",
    "            samples (dict): Batch of samples.\n",
    "\n",
    "        Returns:\n",
    "            dict: Tokenized samples with labels.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer(\n",
    "            samples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",  # \"longest\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return tokens\n",
    "\n",
    "    def train(self, trainer=None):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer, optional): Custom trainer instance.\n",
    "        \"\"\"\n",
    "        trainer = (\n",
    "            Trainer(\n",
    "                model=self.model.model,\n",
    "                args=self.training_args,\n",
    "                train_dataset=self.tokenized_dataset,\n",
    "                data_collator=self.data_collator,\n",
    "                callbacks=[self.liza_callback],\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "            if trainer is None\n",
    "            else trainer\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "class TpttPipeline(Pipeline):\n",
    "    \"\"\"Pipeline for TPTT model inference.\"\"\"\n",
    "\n",
    "    def __init__(self, model: TpttModel):\n",
    "        \"\"\"\n",
    "        Initialize TpttPipeline.\n",
    "\n",
    "        Args:\n",
    "            model (TpttModel): The TPTT model.\n",
    "        \"\"\"\n",
    "        super().__init__(model=model.model, tokenizer=model.tokenizer)\n",
    "        self.model_wrapper = model\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate output from the model given a prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt.\n",
    "            **kwargs: Additional generation arguments.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=kwargs.get(\"max_new_tokens\", 50),\n",
    "                do_sample=kwargs.get(\"do_sample\", False),\n",
    "            )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
