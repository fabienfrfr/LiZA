{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python -V\n!pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:32:09.546298Z","iopub.status.idle":"2025-05-22T21:32:09.546629Z","shell.execute_reply.started":"2025-05-22T21:32:09.546467Z","shell.execute_reply":"2025-05-22T21:32:09.546482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/fabienfrfr/tptt@main","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T21:38:40.420226Z","iopub.execute_input":"2025-05-23T21:38:40.420439Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m925.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for tptt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, DataCollatorForLanguageModeling, TrainerCallback\nfrom datasets import load_dataset\nimport tptt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Load configuration and initialize the TPTT model\n# Using a pretrained backbone (TinyLlama in this example)\nconfig = tptt.TpttConfig(base_model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", mag_weight=0.1, inject_liza=False)\nmodel = tptt.TpttModel(config)\n\n# Step 2: (Optional) Inject LoRA adapters for parameter-efficient fine-tuning\nmodel.add_lora()\n\n# Step 3: Load the tokenizer corresponding to the base model\ntokenizer = AutoTokenizer.from_pretrained(config.base_tokenizer_name)\n# Ensure the tokenizer has a padding token for batching\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token or \"[PAD]\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Prepare the training dataset\n# Here we use a small subset of the Alpaca dataset for demonstration purposes\nraw_dataset = load_dataset(\"yahma/alpaca-cleaned\")[\"train\"].select(range(100))\n\ndef preprocess_fn(samples):\n    \"\"\"\n    Tokenize the samples for causal language modeling.\n    Concatenate instruction, input, and output as needed.\n    \"\"\"\n    prompts = [\n        f\"{instr}\\n{inp}\" if inp else instr\n        for instr, inp in zip(samples[\"instruction\"], samples[\"input\"])\n    ]\n    # Optionally, append output for supervised fine-tuning\n    prompts = [f\"{p}\\n{out}\" for p, out in zip(prompts, samples[\"output\"])]\n    tokens = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",\n        return_attention_mask=True,\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names\n)\n\n# Tokenize the dataset in batches and remove original columns\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Set up a data collator for dynamic padding during training\n#data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\")\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\n\n# Step 6: Define HuggingFace TrainingArguments for reproducible training\ntraining_args = TrainingArguments(\n    output_dir=\"./tptt_output\",\n    per_device_train_batch_size=2, # 2 GPU\n    num_train_epochs=1,\n    learning_rate= 1e-5, #2e-4, too brutal ?\n    max_grad_norm=1.0, # gradiant clipping\n    fp16=True, #fp16=True,  # Use mixed precision if supported by hardware --> doesn't work : return NaN\n    logging_steps=1,\n    save_strategy=\"epoch\",\n    report_to=\"tensorboard\",\n)\n\n# Step 7: Initialize the HuggingFace Trainer\ninitial_weight=0.01,\nfinal_weight=0.5,\ntransition_step=500,\nliza_callback = tptt.AdjustMaGWeightCallback(\n            model,\n            initial_weight=initial_weight,\n            final_weight=final_weight,\n            transition_step=transition_step,)\n\n# Trainer will automatically handle device placement (CPU/GPU)\ntrainer = Trainer(\n    model=model,#.backbone,  # Use the underlying HF model for training\n    args=training_args,\n    #label_names=[\"labels\"],  # (peft warning, but doesn't exist!)\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n    processing_class=tokenizer,\n    #callbacks=[liza_callback],\n    #callbacks=[DebugLossCallback()],\n)\n\n# batch = next(iter(trainer.get_train_dataloader()))\n# print(\"Labels:\", batch[\"labels\"][0]) # Verify if special tokens (-100) for padding\n# print(batch.keys()) # dict_keys(['input_ids', 'attention_mask', 'labels'])\n\n# Step 8: Launch training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Prepare the model for inference\n# Move model to the desired device (e.g., \"cuda:0\" or \"cpu\") for generation\ndevice = 0 if torch.cuda.is_available() else -1\nmodel.backbone.to(f\"cuda:{device}\" if device != -1 else \"cpu\")\n\n# Step 10: Build the inference pipeline with the correct device and tokenizer\npipe = tptt.TpttPipeline(model=model.backbone, tokenizer=tokenizer, device=device)\n\n# Step 11: Generate text from a prompt\nresult = pipe(\"Once upon a time,\", max_new_tokens=150)\nprint(result[0][\"generated_text\"])  # Print the generated text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:14:00.439958Z","iopub.execute_input":"2025-05-23T20:14:00.440253Z","iopub.status.idle":"2025-05-23T20:14:05.019427Z","shell.execute_reply.started":"2025-05-23T20:14:00.440235Z","shell.execute_reply":"2025-05-23T20:14:05.018538Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time,\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Step 12: Save the trained model and tokenizer for future use or deployment\nmodel.save_pretrained(\"./my_tptt_model\")\ntokenizer.save_pretrained(\"./my_tptt_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### DEBUG PARTS","metadata":{}},{"cell_type":"code","source":"### DEBUG Callback\nclass DebugLossCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None and \"loss\" in logs:\n            print(f\"[DEBUG][Callback] Step {state.global_step} loss: {logs['loss']}\")\n\nbatch = next(iter(trainer.get_train_dataloader()))\nprint(\"\\n[DEBUG] Batch keys:\", batch.keys())\nprint(\"[DEBUG] input_ids (first 10):\", batch[\"input_ids\"][0][:10])\nprint(\"[DEBUG] labels (first 10):\", batch[\"labels\"][0][:10])\nprint(\"[DEBUG] labels unique:\", torch.unique(batch[\"labels\"]))\nprint(\"[DEBUG] nb non-masked labels:\", (batch[\"labels\"] != -100).sum().item())\noutput = model(**batch)\nprint(\"[DEBUG] Forward output.loss:\", output.loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:33:47.491665Z","iopub.execute_input":"2025-05-23T20:33:47.491969Z","iopub.status.idle":"2025-05-23T20:33:48.375056Z","shell.execute_reply.started":"2025-05-23T20:33:47.491945Z","shell.execute_reply":"2025-05-23T20:33:48.374461Z"}},"outputs":[{"name":"stdout","text":"\n[DEBUG] Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n[DEBUG] input_ids (first 10): tensor([    1, 12027,  7420,  2020,   278,  2183,  5023,   338,  2743, 29889],\n       device='cuda:0')\n[DEBUG] labels (first 10): tensor([    1, 12027,  7420,  2020,   278,  2183,  5023,   338,  2743, 29889],\n       device='cuda:0')\n[DEBUG] labels unique: tensor([ -100,     1,    13,   263,   278,   286,   287,   289,   292,   297,\n          304,   310,   322,   338,   339,   341,   353,   363,   367,   372,\n          385,   393,   394,   403,   408,   411,   421,   440,   445,   450,\n          451,   470,   471,   472,   488,   491,   508,   512,   513,   515,\n          526,   573,   607,   798,   884,   896,   921,   935,   947,   964,\n          967,  1009,  1033,  1090,  1152,  1206,  1209,  1304,  1338,  1363,\n         1412,  1438,  1494,  1565,  1576,  1598,  1840,  2000,  2020,  2057,\n         2089,  2183,  2319,  2330,  2485,  2486,  2688,  2743,  2920,  2998,\n         3148,  3229,  3273,  3309,  3588,  3898,  3942,  4038,  4134,  4768,\n         4850,  4912,  5023,  5076,  5190,  5574,  5824,  5890,  5996,  6020,\n         6544,  6635,  6674,  6852,  6866,  7314,  7420,  7477,  7857,  8453,\n         8809,  8950,  8973,  9186,  9245,  9337,  9436,  9590,  9939, 10288,\n        10541, 11203, 11221, 11796, 12027, 12080, 12833, 12965, 13019, 13206,\n        13391, 14637, 16232, 16701, 16888, 17473, 17564, 17873, 19818, 19967,\n        20039, 20396, 20657, 21862, 22233, 22569, 22827, 28907, 29037, 29871,\n        29872, 29879, 29886, 29889, 29892, 29896, 29900, 29909, 29915, 29945,\n        30088], device='cuda:0')\n[DEBUG] nb non-masked labels: 312\n[DEBUG] Forward output.loss: tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":6}]}